{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Argus-PBT-Multi-NonPrallelized.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qyqlKledXePq"
      },
      "source": [
        "# Shit needed to make this thing work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC1Xvqt0KGwh"
      },
      "source": [
        "%%capture\n",
        "!git clone https://github.com/koulanurag/ma-gym.git\n",
        "%cd /content/ma-gym/\n",
        "!pip install -e .\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTfQUesOLi2F"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import tensorflow_probability as tfp\n",
        "import threading\n",
        "import random\n",
        "import numpy as np\n",
        "import gym\n",
        "import ma_gym\n",
        "import copy\n",
        "from tensorflow.keras import backend as K"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0sSHLdgfXiJ_"
      },
      "source": [
        "# Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-objPCqWCZv"
      },
      "source": [
        "class ActorNetwork(tf.keras.Model):\n",
        "    def __init__(self, output_dims, id):\n",
        "        super(ActorNetwork, self).__init__()\n",
        "        self.output_dims = output_dims\n",
        "        # Create a checkpoint directory in case we want to save our model\n",
        "        name = 'Actor'\n",
        "        self.model_name = name + f' {id}'\n",
        "\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\n",
        "        self.checkpoint_dir = checkpoint_directory\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\n",
        "\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.action_probs = tf.keras.layers.Dense(units=self.output_dims, activation=None, name='Action_Logits',\n",
        "                                                  dtype=tf.float64)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense_layer_1(state)\n",
        "        x = self.dense_layer_2(x)\n",
        "        x = self.dense_layer_3(x)\n",
        "        action_probs = self.action_probs(x)\n",
        "        return action_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ApZ_PHOXlTF"
      },
      "source": [
        "# Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVI1o3JXTOU"
      },
      "source": [
        "class CriticNetwork(tf.keras.Model):\n",
        "    def __init__(self, output_dims, id):\n",
        "        super(CriticNetwork, self).__init__()\n",
        "        self.output_dims = output_dims\n",
        "        # Create a checkpoint directory in case we want to save our model\n",
        "        name = 'Critic'\n",
        "        self.model_name = name + f' {id}'\n",
        "\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\n",
        "        self.checkpoint_dir = checkpoint_directory\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\n",
        "\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\n",
        "                                                   dtype=tf.float64)\n",
        "        self.state_value = tf.keras.layers.Dense(units=1, activation=None, name='State_Value',\n",
        "                                                 dtype=tf.float64)\n",
        "\n",
        "    def call(self, state):\n",
        "        x = self.dense_layer_1(state)\n",
        "        x = self.dense_layer_2(x)\n",
        "        x = self.dense_layer_3(x)\n",
        "        state_value = self.state_value(x)\n",
        "        return state_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISzuGk1c6nIu"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Yi0U0-qXTcu"
      },
      "source": [
        "class Agent:\n",
        "    def __init__(self, output_dims, initial_hyper_parameters, id):\n",
        "        # Agent's parameters needed for logging\n",
        "        self.id = id\n",
        "        self.cum_sum = 0\n",
        "        self.episode_num = 0\n",
        "\n",
        "        # Agent's initial hyper-parameters\n",
        "        self.hyper_parameters = initial_hyper_parameters\n",
        "\n",
        "        # These are the parameters we want to use with population based training\n",
        "        self.actor_learning_rate = self.hyper_parameters['actor_learning_rate']\n",
        "        self.critic_learning_rate = self.hyper_parameters['critic_learning_rate']\n",
        "\n",
        "        # We're going to use one network for all of our minions\n",
        "        self.actor_network = ActorNetwork(output_dims=output_dims, id=self.id)\n",
        "        self.critic_network = CriticNetwork(output_dims=1, id=self.id)\n",
        "\n",
        "        self.actor_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.actor_learning_rate))\n",
        "        self.critic_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.critic_learning_rate))\n",
        "\n",
        "        # Since Actor-Critic is an on-policy method, we will not use a replay buffer\n",
        "        self.states = []\n",
        "        self.actions = []\n",
        "        self.rewards = []\n",
        "        self.episode_rewards = []\n",
        "        self.scores = []\n",
        "        self.actor_losses = []\n",
        "        self.critic_losses = []\n",
        "\n",
        "    def save_models(self):\n",
        "        # print('... saving models ...')\n",
        "        self.actor_network.save_weights(self.actor_network.checkpoint_file)\n",
        "        self.critic_network.save_weights(self.critic_network.checkpoint_file)\n",
        "\n",
        "    def load_models(self):\n",
        "        # print('... loading models ...')\n",
        "        self.actor_network.load_weights(self.actor_network.checkpoint_file)\n",
        "        self.critic_network.load_weights(self.critic_network.checkpoint_file)\n",
        "\n",
        "    def choose_action(self, state):\n",
        "        action_logits = self.actor_network(tf.convert_to_tensor([state]))\n",
        "        action_probabilities = tf.nn.softmax(action_logits)\n",
        "        action_distribution = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\n",
        "        action = action_distribution.sample()\n",
        "\n",
        "        return int(action.numpy()[0])\n",
        "\n",
        "    def learn(self):\n",
        "        discounted_rewards = []\n",
        "        sum_reward = 0\n",
        "        self.rewards.reverse()\n",
        "        for r in self.rewards:\n",
        "            sum_reward = r + self.hyper_parameters['discount_factor'] * sum_reward\n",
        "            discounted_rewards.append(sum_reward)\n",
        "        discounted_rewards.reverse()\n",
        "\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\n",
        "            # Start calculating the Actor and Critic losses for each minion's experience\n",
        "            action_logits = self.actor_network(tf.convert_to_tensor(self.states))\n",
        "            state_values = self.critic_network(tf.convert_to_tensor(self.states))\n",
        "            action_probabilities = tf.nn.softmax(action_logits)\n",
        "            # We'll be using an advantage function\n",
        "            action_distributions = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\n",
        "            log_probs = action_distributions.log_prob(self.actions)\n",
        "            advantage = tf.math.subtract(discounted_rewards, state_values)\n",
        "            entropy = -1 * tf.math.reduce_sum(action_probabilities * tf.math.log(action_probabilities))\n",
        "            actor_loss = tf.math.reduce_mean(-1 * log_probs * advantage) - self.hyper_parameters[\n",
        "                'entropy_coefficient'] * entropy\n",
        "            critic_loss = tf.math.reduce_mean(advantage ** 2)\n",
        "\n",
        "            # Optimize master's network with the mean of all the losses\n",
        "        actor_grads = tape1.gradient(actor_loss, self.actor_network.trainable_variables)\n",
        "        critic_grads = tape2.gradient(critic_loss, self.critic_network.trainable_variables)\n",
        "        self.actor_network.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
        "        self.critic_network.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
        "        self.actor_losses.append(actor_loss.numpy())\n",
        "        self.critic_losses.append(critic_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vY53o7yEYrIB"
      },
      "source": [
        "# Coordinator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YolGL-m9XYdg"
      },
      "source": [
        "class Coordinator:\n",
        "    def __init__(self, environment_name, initial_hyper_parameters, coordinator_id, log_file_name):\n",
        "        self.environment_name = environment_name\n",
        "        self.id = coordinator_id\n",
        "        self.log_file_name = log_file_name\n",
        "        self.environment = gym.make(environment_name)\n",
        "        self.observation = self.environment.reset()\n",
        "        self.number_of_agents = len(self.environment.get_action_meanings())\n",
        "        self.output_dims = len(self.environment.get_action_meanings()[0])\n",
        "        self.observation_dims = self.environment.observation_space[0].shape[0]\n",
        "\n",
        "        self.hyper_parameters = initial_hyper_parameters\n",
        "        self.agents = [Agent(self.output_dims, self.hyper_parameters, id=i) for i in range(self.number_of_agents)]\n",
        "        self.episode_finished = False\n",
        "        self.episode_num=0\n",
        "        self.mean_scores = 0\n",
        "        self.episode_number = 0\n",
        "        self.episode_rewards = []\n",
        "        self.total_reward = 0\n",
        "        self.episode_finished = False\n",
        "\n",
        "    def play(self, show_env=False):\n",
        "        self.episode_finished = False\n",
        "        steps = 0\n",
        "\n",
        "        while not self.episode_finished:\n",
        "            actions = []\n",
        "\n",
        "            for i, observation in enumerate(self.observation):\n",
        "                self.agents[i].states.append(observation)\n",
        "                temp_action = self.agents[i].choose_action(observation)\n",
        "                self.agents[i].actions.append(temp_action)\n",
        "                actions.append(temp_action)\n",
        "\n",
        "            next_state, rewards, dones, info = self.environment.step(actions)\n",
        "            \n",
        "            for i, reward in enumerate(rewards):\n",
        "                if reward == -1.01:\n",
        "                    reward = 4.99\n",
        "                    rewards[i] = 4.99\n",
        "                if reward == 0.99:\n",
        "                    reward = -5.01\n",
        "                    rewards[i] = -5.01\n",
        "                # if reward not in self.distinct_rewards:\n",
        "                #     self.distinct_rewards.append(reward)\n",
        "                self.agents[i].rewards.append(reward)\n",
        "                self.total_reward += reward\n",
        "\n",
        "\n",
        "            # if show_env:\n",
        "            # self.environment.render()\n",
        "\n",
        "            self.observation = next_state\n",
        "\n",
        "            if dones == [True for i in range(len(self.agents))]:\n",
        "                self.episode_finished = True\n",
        "                self.episode_number += 1\n",
        "                self.observation = self.environment.reset()\n",
        "                self.episode_rewards.append(self.total_reward)\n",
        "                self.mean_scores = np.mean(self.episode_rewards)\n",
        "                for agent in self.agents:\n",
        "                    f = open(f'{self.environment_name}-{self.log_file_name}.csv', 'a')\n",
        "                    f.write(f'{self.id},{self.episode_number},{agent.id},{np.sum(agent.rewards)},{self.total_reward},{agent.hyper_parameters[\"actor_learning_rate\"]},{agent.hyper_parameters[\"critic_learning_rate\"]},{agent.hyper_parameters[\"entropy_coefficient\"]}\\n')\n",
        "                    f.close()\n",
        "                self.total_reward = 0\n",
        "                self.episode_num+=1\n",
        "\n",
        "            steps += 1\n",
        "\n",
        "        for agent in self.agents:\n",
        "            agent.learn()\n",
        "            agent.states.clear()\n",
        "            agent.rewards.clear()\n",
        "            agent.actions.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpr7evycYo0m"
      },
      "source": [
        "# PBT Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYKTSuY1XV-v"
      },
      "source": [
        "def exploit(population):\n",
        "    sorted_population = sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)\n",
        "    best_coordinators = sorted_population[:3]\n",
        "    worst_coordinators = sorted_population[-3:]\n",
        "\n",
        "    # for each other agent, load their models here\n",
        "    for coordinator in worst_coordinators:\n",
        "        worst_coordinator_id = coordinator.id\n",
        "        worst_coordinator_episode = coordinator.episode_num\n",
        "        new_coordinator = copy.deepcopy(random.choice(best_coordinators))\n",
        "        print(f'Agent -> {new_coordinator.id} will replace {worst_coordinator_id}')\n",
        "        new_coordinator.id = worst_coordinator_id\n",
        "        new_coordinator.episode_num = worst_coordinator_episode\n",
        "        population.remove(coordinator)\n",
        "        population.append(new_coordinator)\n",
        "        explore(new_coordinator)\n",
        "    \n",
        "    for coordinator in population:\n",
        "        coordinator.episode_rewards.clear()\n",
        "\n",
        "\n",
        "def explore(coordinator):\n",
        "  for agent in coordinator.agents:\n",
        "      new_actor_learning_rate = round(agent.hyper_parameters['actor_learning_rate'] * random.uniform(0.8, 1.2), 6)\n",
        "      new_critic_learning_rate = round(agent.hyper_parameters['critic_learning_rate'] * random.uniform(0.8, 1.2), 6)\n",
        "      # new_entropy_coefficient = round(agent.hyper_parameters['entropy_coefficient'] * random.uniform(0.8, 1.2), 0)\n",
        "\n",
        "      # new_discount_factor = round(best_agent.hyper_parameters['discount_factor'] * random.uniform(0.8, 1.2), 2)\n",
        "      # if new_discount_factor > 1:\n",
        "      #     new_discount_factor = 1\n",
        "\n",
        "      agent.actor_network.optimizer.learning_rate.assign(new_actor_learning_rate)\n",
        "      agent.critic_network.optimizer.learning_rate.assign(new_critic_learning_rate)\n",
        "      agent.hyper_parameters['actor_learning_rate'] = new_actor_learning_rate\n",
        "      agent.hyper_parameters['critic_learning_rate'] = new_critic_learning_rate\n",
        "      # agent.hyper_parameters['entropy_coefficient'] = new_entropy_coefficient\n",
        "      # agent.hyper_parameters['discount_factor'] = new_discount_factor\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eCwkgSdth1F"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "de_dkkVtonhJ"
      },
      "source": [
        "environment_name = 'Checkers-v0'\n",
        "log_file_name = 'PBT 2'\n",
        "population=[]\n",
        "for i in range(16):\n",
        "  population.append( Coordinator(environment_name,\n",
        "                            {'actor_learning_rate': round(random.uniform(0.00001,0.01),4),\n",
        "                              'critic_learning_rate': round(random.uniform(0.00001,0.01),4),\n",
        "                              'entropy_coefficient': 0.001,\n",
        "                              'critic_coefficient': 0.3,\n",
        "                              'discount_factor': 0.99,\n",
        "                              'unroll_length': 5,\n",
        "                              'minions_num': 5},\n",
        "                            coordinator_id=i,log_file_name=log_file_name))\n",
        "f = open(f'{environment_name}-{log_file_name}.csv', 'a')\n",
        "f.write(f'Coordinator ID,Episode Number,Agent ID,Agent Reward,Episode Reward, Actor Learning Rate, Critic Learning Rate, Entropy\\n')\n",
        "f.close()\n",
        "j=0\n",
        "\n",
        "for j in range(1,2000):\n",
        "    for coordinator in population:\n",
        "      try:\n",
        "        coordinator.play()\n",
        "      except Exception:\n",
        "        new_coordinator_id = coordinator.id\n",
        "        new_coordinator_episode = coordinator.episode_num\n",
        "        population.remove(coordinator)\n",
        "        new_coordinator = copy.deepcopy(random.choice(sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)[:3]))\n",
        "        new_coordinator.id = new_coordinator_id\n",
        "        new_coordinator.episode_num = new_coordinator_episode\n",
        "        population.append(new_coordinator)\n",
        "  \n",
        "    if j%100==0:\n",
        "      for coordinator in population:\n",
        "          print(f'{coordinator.id} --> {coordinator.episode_number} --> {np.mean(coordinator.episode_rewards)}')\n",
        "      exploit(population)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bx40M5nlu2B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6984e9b6-6e8a-4b72-b79b-6ceefce3dc3f"
      },
      "source": [
        "from google.colab import files\r\n",
        "files.download(f'/content/ma-gym/{environment_name}-{log_file_name}.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ca3cbfe5-8d7c-4bdf-81f9-feadfa53b17b\", \"Checkers-v0-PBT 2.csv\", 4379358)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}