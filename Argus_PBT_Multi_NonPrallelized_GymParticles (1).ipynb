{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Argus-PBT-Multi-NonPrallelized-GymParticles.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eATIzxDhP4FS"
      },
      "source": [
        "# Shit needed to make this thing work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRhVmreAQOsp"
      },
      "source": [
        "## Setting up Gym Particles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rs7BheCPwXV"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pettingzoo[mpe]\r\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wjr91MLQwAx"
      },
      "source": [
        "## Needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yA712aCQ2P4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import tensorflow_probability as tfp\r\n",
        "import threading\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import gym\r\n",
        "import copy\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from pettingzoo.mpe import simple_adversary_v2\r\n",
        "from google.colab import files\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdtZHA1S8U9"
      },
      "source": [
        "# Actor, Critic Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoXccYLVTEHV"
      },
      "source": [
        "## Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEnUw44tSroH"
      },
      "source": [
        "class ActorNetwork(tf.keras.Model):\r\n",
        "    def __init__(self, output_dims, id):\r\n",
        "        super(ActorNetwork, self).__init__()\r\n",
        "        self.output_dims = output_dims\r\n",
        "        # Create a checkpoint directory in case we want to save our model\r\n",
        "        name = 'Actor'\r\n",
        "        self.model_name = name + f' {id}'\r\n",
        "\r\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\r\n",
        "        self.checkpoint_dir = checkpoint_directory\r\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\r\n",
        "\r\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.action_probs = tf.keras.layers.Dense(units=self.output_dims, activation=None, name='Action_Logits',\r\n",
        "                                                  dtype=tf.float64)\r\n",
        "\r\n",
        "    def call(self, state):\r\n",
        "        x = self.dense_layer_1(state)\r\n",
        "        x = self.dense_layer_2(x)\r\n",
        "        x = self.dense_layer_3(x)\r\n",
        "        action_probs = self.action_probs(x)\r\n",
        "        return action_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMe7RnKXTIj-"
      },
      "source": [
        "## Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip6wQIPPTK5Y"
      },
      "source": [
        "class CriticNetwork(tf.keras.Model):\r\n",
        "    def __init__(self, output_dims, id):\r\n",
        "        super(CriticNetwork, self).__init__()\r\n",
        "        self.output_dims = output_dims\r\n",
        "        # Create a checkpoint directory in case we want to save our model\r\n",
        "        name = 'Critic'\r\n",
        "        self.model_name = name + f' {id}'\r\n",
        "\r\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\r\n",
        "        self.checkpoint_dir = checkpoint_directory\r\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\r\n",
        "\r\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.state_value = tf.keras.layers.Dense(units=1, activation=None, name='State_Value',\r\n",
        "                                                 dtype=tf.float64)\r\n",
        "\r\n",
        "    def call(self, state):\r\n",
        "        x = self.dense_layer_1(state)\r\n",
        "        x = self.dense_layer_2(x)\r\n",
        "        x = self.dense_layer_3(x)\r\n",
        "        state_value = self.state_value(x)\r\n",
        "        return state_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xIbUIemTN_-"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgRGQ8zTP22"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self, output_dims, initial_hyper_parameters, id):\r\n",
        "        # Agent's parameters needed for logging\r\n",
        "        self.id = id\r\n",
        "        self.cum_sum = 0\r\n",
        "        self.episode_num = 0\r\n",
        "\r\n",
        "        # Agent's initial hyper-parameters\r\n",
        "        self.hyper_parameters = initial_hyper_parameters\r\n",
        "\r\n",
        "        # These are the parameters we want to use with population based training\r\n",
        "        self.actor_learning_rate = self.hyper_parameters['actor_learning_rate']\r\n",
        "        self.critic_learning_rate = self.hyper_parameters['critic_learning_rate']\r\n",
        "\r\n",
        "        # We're going to use one network for all of our minions\r\n",
        "        self.actor_network = ActorNetwork(output_dims=output_dims, id=self.id)\r\n",
        "        self.critic_network = CriticNetwork(output_dims=1, id=self.id)\r\n",
        "\r\n",
        "        self.actor_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.actor_learning_rate))\r\n",
        "        self.critic_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.critic_learning_rate))\r\n",
        "\r\n",
        "        # Since Actor-Critic is an on-policy method, we will not use a replay buffer\r\n",
        "        self.states = []\r\n",
        "        self.actions = []\r\n",
        "        self.rewards = []\r\n",
        "        self.episode_rewards = []\r\n",
        "        self.scores = []\r\n",
        "        self.actor_losses = []\r\n",
        "        self.critic_losses = []\r\n",
        "\r\n",
        "    def save_models(self):\r\n",
        "        # print('... saving models ...')\r\n",
        "        self.actor_network.save_weights(self.actor_network.checkpoint_file)\r\n",
        "        self.critic_network.save_weights(self.critic_network.checkpoint_file)\r\n",
        "\r\n",
        "    def load_models(self):\r\n",
        "        # print('... loading models ...')\r\n",
        "        self.actor_network.load_weights(self.actor_network.checkpoint_file)\r\n",
        "        self.critic_network.load_weights(self.critic_network.checkpoint_file)\r\n",
        "\r\n",
        "    def choose_action(self, state):\r\n",
        "        action_logits = self.actor_network(tf.convert_to_tensor([state]))\r\n",
        "        action_probabilities = tf.nn.softmax(action_logits)\r\n",
        "        action_distribution = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\r\n",
        "        action = action_distribution.sample()\r\n",
        "\r\n",
        "        return int(action.numpy()[0])\r\n",
        "\r\n",
        "    def learn(self):\r\n",
        "        discounted_rewards = []\r\n",
        "        sum_reward = 0\r\n",
        "        self.rewards.reverse()\r\n",
        "        for r in self.rewards:\r\n",
        "            sum_reward = r + self.hyper_parameters['discount_factor'] * sum_reward\r\n",
        "            discounted_rewards.append(sum_reward)\r\n",
        "        discounted_rewards.reverse()\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\r\n",
        "            # Start calculating the Actor and Critic losses for each minion's experience\r\n",
        "            action_logits = self.actor_network(tf.convert_to_tensor(self.states))\r\n",
        "            state_values = self.critic_network(tf.convert_to_tensor(self.states))\r\n",
        "            action_probabilities = tf.nn.softmax(action_logits)\r\n",
        "            # We'll be using an advantage function\r\n",
        "            action_distributions = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\r\n",
        "            log_probs = action_distributions.log_prob(self.actions)\r\n",
        "            advantage = tf.math.subtract(discounted_rewards, state_values)\r\n",
        "            entropy = -1 * tf.math.reduce_sum(action_probabilities * tf.math.log(action_probabilities))\r\n",
        "            actor_loss = tf.math.reduce_mean(-1 * log_probs * advantage) - self.hyper_parameters[\r\n",
        "                'entropy_coefficient'] * entropy\r\n",
        "            critic_loss = tf.math.reduce_mean(advantage ** 2)\r\n",
        "\r\n",
        "            # Optimize master's network with the mean of all the losses\r\n",
        "        actor_grads = tape1.gradient(actor_loss, self.actor_network.trainable_variables)\r\n",
        "        critic_grads = tape2.gradient(critic_loss, self.critic_network.trainable_variables)\r\n",
        "        self.actor_network.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\r\n",
        "        self.critic_network.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\r\n",
        "        self.actor_losses.append(actor_loss.numpy())\r\n",
        "        self.critic_losses.append(critic_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewaKLDdzTmAi"
      },
      "source": [
        "# Coordinator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkI73wBFTowU"
      },
      "source": [
        "class Coordinator:\r\n",
        "    def __init__(self, environment_name, initial_hyper_parameters, coordinator_id, log_file_name):\r\n",
        "        self.environment_name = environment_name\r\n",
        "        self.id = coordinator_id\r\n",
        "        self.log_file_name = log_file_name\r\n",
        "        self.environment = simple_adversary_v2.env(N=2, max_cycles=100)\r\n",
        "        self.observation = self.environment.reset()\r\n",
        "        self.number_of_agents = len(self.environment.agents)\r\n",
        "\r\n",
        "        self.hyper_parameters = initial_hyper_parameters\r\n",
        "        self.agents = [Agent(self.environment.action_spaces[i].n, self.hyper_parameters, id=i) for i in\r\n",
        "                       self.environment.action_spaces.keys()]\r\n",
        "\r\n",
        "        self.episode_finished = False\r\n",
        "        self.episode_num = 0\r\n",
        "        self.mean_scores = 0\r\n",
        "        self.episode_number = 0\r\n",
        "        self.episode_rewards = []\r\n",
        "        self.total_reward = 0\r\n",
        "        self.episode_finished = False\r\n",
        "        self.steps = 1\r\n",
        "\r\n",
        "    def play(self, show_env=False):\r\n",
        "        self.episode_finished = False\r\n",
        "        self.environment.reset()\r\n",
        "        while not all(done == True for done in self.environment.dones):\r\n",
        "            i = 0\r\n",
        "            for agent in self.environment.agent_iter():\r\n",
        "\r\n",
        "                agent_obs, agent_reward, agent_done, agent_info = self.environment.last()\r\n",
        "                self.agents[i].rewards.append(agent_reward)\r\n",
        "\r\n",
        "                self.agents[i].states.append(agent_obs)\r\n",
        "                temp_action = self.agents[i].choose_action(agent_obs)\r\n",
        "\r\n",
        "                if agent_done:\r\n",
        "                    self.agents[i].actions.append(0)\r\n",
        "                    self.environment.step(None)\r\n",
        "                else:\r\n",
        "                    self.agents[i].actions.append(temp_action)\r\n",
        "                    self.environment.step(temp_action)\r\n",
        "\r\n",
        "                if show_env:\r\n",
        "                    self.environment.render(mode='human')\r\n",
        "                i += 1\r\n",
        "                if i == self.number_of_agents: i = 0\r\n",
        "\r\n",
        "        self.episode_num += 1\r\n",
        "        total_episode_reward = 0\r\n",
        "        for agent in self.agents[:1]:\r\n",
        "            total_episode_reward += np.sum(agent.rewards)\r\n",
        "        coordinator.episode_rewards.append(total_episode_reward)\r\n",
        "        for agent in self.agents:\r\n",
        "            f = open(f'{self.environment_name}-{self.log_file_name}.csv', 'a')\r\n",
        "            f.write(\r\n",
        "                f'{self.id},{self.episode_num},{total_episode_reward},{agent.id},{np.sum(agent.rewards)},'\r\n",
        "                f'{agent.hyper_parameters[\"actor_learning_rate\"]},{agent.hyper_parameters[\"critic_learning_rate\"]},'\r\n",
        "                f'{agent.hyper_parameters[\"entropy_coefficient\"]}\\n')\r\n",
        "            f.close()\r\n",
        "\r\n",
        "        for agent in self.agents:\r\n",
        "            agent.learn()\r\n",
        "            agent.states.clear()\r\n",
        "            agent.rewards.clear()\r\n",
        "            agent.actions.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpFOf37kT0cg"
      },
      "source": [
        "# PBT Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUSV6v5T3Qt"
      },
      "source": [
        "def exploit(population):\r\n",
        "    sorted_population = sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)\r\n",
        "    best_coordinators = sorted_population[:3]\r\n",
        "    worst_coordinators = sorted_population[-3:]\r\n",
        "\r\n",
        "    # for each other agent, load their models here\r\n",
        "    for coordinator in worst_coordinators:\r\n",
        "        worst_coordinator_id = coordinator.id\r\n",
        "        worst_coordinator_episode = coordinator.episode_num\r\n",
        "        new_coordinator = copy.deepcopy(random.choice(best_coordinators))\r\n",
        "        print(f'Agent -> {new_coordinator.id} will replace {worst_coordinator_id}')\r\n",
        "        new_coordinator.id = worst_coordinator_id\r\n",
        "        new_coordinator.episode_num = worst_coordinator_episode\r\n",
        "        population.remove(coordinator)\r\n",
        "        population.append(new_coordinator)\r\n",
        "        explore(new_coordinator)\r\n",
        "\r\n",
        "    for coordinator in population:\r\n",
        "        coordinator.episode_rewards.clear()\r\n",
        "\r\n",
        "\r\n",
        "def explore(coordinator):\r\n",
        "    for agent in coordinator.agents:\r\n",
        "        new_actor_learning_rate = round(agent.hyper_parameters['actor_learning_rate'] * random.uniform(0.8, 1.2), 6)\r\n",
        "        new_critic_learning_rate = round(agent.hyper_parameters['critic_learning_rate'] * random.uniform(0.8, 1.2), 6)\r\n",
        "\r\n",
        "        agent.actor_network.optimizer.learning_rate.assign(new_actor_learning_rate)\r\n",
        "        agent.critic_network.optimizer.learning_rate.assign(new_critic_learning_rate)\r\n",
        "        agent.hyper_parameters['actor_learning_rate'] = new_actor_learning_rate\r\n",
        "        agent.hyper_parameters['critic_learning_rate'] = new_critic_learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KebZ1Qr4T6Tw"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4DRcPJLT8Vv"
      },
      "source": [
        "environment_name = 'Simple Adevrsary'\r\n",
        "log_file_name = 'PBT-5'\r\n",
        "population = []\r\n",
        "for i in range(16):\r\n",
        "    population.append(Coordinator(environment_name,\r\n",
        "                                  {'actor_learning_rate': round(random.uniform(0.00001, 0.001), 4),\r\n",
        "                                   'critic_learning_rate': round(random.uniform(0.00001, 0.001), 4),\r\n",
        "                                   'entropy_coefficient': 0.0001,\r\n",
        "                                   'critic_coefficient': 0.3,\r\n",
        "                                   'discount_factor': 0.95,\r\n",
        "                                   'unroll_length': 5,\r\n",
        "                                   'minions_num': 5},\r\n",
        "                                  coordinator_id=i, log_file_name=log_file_name))\r\n",
        "f = open(f'{environment_name}-{log_file_name}.csv', 'a')\r\n",
        "f.write(\r\n",
        "    f'Coordinator ID,Episode Number,Episode Reward,Agent ID,Agent Reward, Actor Learning Rate, Critic Learning Rate, '\r\n",
        "    f'Entropy\\n')\r\n",
        "f.close()\r\n",
        "j = 0\r\n",
        "\r\n",
        "for j in range(1, 1501):\r\n",
        "    for coordinator in population:\r\n",
        "        try:\r\n",
        "          coordinator.play(show_env=False)\r\n",
        "        except Exception:\r\n",
        "            new_coordinator_id = coordinator.id\r\n",
        "            new_coordinator_episode = coordinator.episode_num\r\n",
        "            population.remove(coordinator)\r\n",
        "            new_coordinator = copy.deepcopy(\r\n",
        "                random.choice(sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)[:3]))\r\n",
        "            new_coordinator.id = new_coordinator_id\r\n",
        "            new_coordinator.episode_num = new_coordinator_episode\r\n",
        "            population.append(new_coordinator)\r\n",
        "\r\n",
        "    if j % 100 == 0:\r\n",
        "        for coordinator in population:\r\n",
        "            print(f'{coordinator.id} --> {coordinator.episode_num} --> {np.mean(coordinator.episode_rewards)}')\r\n",
        "        exploit(population)\r\n",
        "        # files.download(f'{environment_name}-{log_file_name}.csv')\r\n",
        "\r\n",
        "files.download(f'{environment_name}-{log_file_name}.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}