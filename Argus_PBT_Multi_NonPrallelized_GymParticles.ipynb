{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Argus-PBT-Multi-NonPrallelized-GymParticles.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eATIzxDhP4FS"
      },
      "source": [
        "# Shit needed to make this thing work"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRhVmreAQOsp"
      },
      "source": [
        "## Setting up Gym Particles"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rs7BheCPwXV"
      },
      "source": [
        "%%capture\r\n",
        "!pip install pettingzoo[mpe]\r\n",
        "!pip3 install box2d-py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wjr91MLQwAx"
      },
      "source": [
        "## Needed Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5yA712aCQ2P4"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import tensorflow_probability as tfp\r\n",
        "import threading\r\n",
        "import random\r\n",
        "import numpy as np\r\n",
        "import gym\r\n",
        "import copy\r\n",
        "from tensorflow.keras import backend as K\r\n",
        "from pettingzoo.mpe import simple_adversary_v2\r\n",
        "from google.colab import files\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJdtZHA1S8U9"
      },
      "source": [
        "# Actor, Critic Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoXccYLVTEHV"
      },
      "source": [
        "## Actor Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEnUw44tSroH"
      },
      "source": [
        "class ActorNetwork(tf.keras.Model):\r\n",
        "    def __init__(self, output_dims, id):\r\n",
        "        super(ActorNetwork, self).__init__()\r\n",
        "        self.output_dims = output_dims\r\n",
        "        # Create a checkpoint directory in case we want to save our model\r\n",
        "        name = 'Actor'\r\n",
        "        self.model_name = name + f' {id}'\r\n",
        "\r\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\r\n",
        "        self.checkpoint_dir = checkpoint_directory\r\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\r\n",
        "\r\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.action_probs = tf.keras.layers.Dense(units=self.output_dims, activation=None, name='Action_Logits',\r\n",
        "                                                  dtype=tf.float64)\r\n",
        "\r\n",
        "    def call(self, state):\r\n",
        "        x = self.dense_layer_1(state)\r\n",
        "        x = self.dense_layer_2(x)\r\n",
        "        x = self.dense_layer_3(x)\r\n",
        "        action_probs = self.action_probs(x)\r\n",
        "        return action_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMe7RnKXTIj-"
      },
      "source": [
        "## Critic Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip6wQIPPTK5Y"
      },
      "source": [
        "class CriticNetwork(tf.keras.Model):\r\n",
        "    def __init__(self, output_dims, id):\r\n",
        "        super(CriticNetwork, self).__init__()\r\n",
        "        self.output_dims = output_dims\r\n",
        "        # Create a checkpoint directory in case we want to save our model\r\n",
        "        name = 'Critic'\r\n",
        "        self.model_name = name + f' {id}'\r\n",
        "\r\n",
        "        checkpoint_directory = f'{os.getcwd()}//Agent Models'\r\n",
        "        self.checkpoint_dir = checkpoint_directory\r\n",
        "        self.checkpoint_file = os.path.join(self.checkpoint_dir, self.model_name + '.h5')\r\n",
        "\r\n",
        "        self.dense_layer_1 = tf.keras.layers.Dense(units=2048, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_2 = tf.keras.layers.Dense(units=1024, activation='relu', name='Dense_Layer_2',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.dense_layer_3 = tf.keras.layers.Dense(units=512, activation='relu', name='Dense_Layer_1',\r\n",
        "                                                   dtype=tf.float64)\r\n",
        "        self.state_value = tf.keras.layers.Dense(units=1, activation=None, name='State_Value',\r\n",
        "                                                 dtype=tf.float64)\r\n",
        "\r\n",
        "    def call(self, state):\r\n",
        "        x = self.dense_layer_1(state)\r\n",
        "        x = self.dense_layer_2(x)\r\n",
        "        x = self.dense_layer_3(x)\r\n",
        "        state_value = self.state_value(x)\r\n",
        "        return state_value"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xIbUIemTN_-"
      },
      "source": [
        "# Agent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgRGQ8zTP22"
      },
      "source": [
        "class Agent:\r\n",
        "    def __init__(self, output_dims, initial_hyper_parameters, id):\r\n",
        "        # Agent's parameters needed for logging\r\n",
        "        self.id = id\r\n",
        "        self.cum_sum = 0\r\n",
        "        self.episode_num = 0\r\n",
        "\r\n",
        "        # Agent's initial hyper-parameters\r\n",
        "        self.hyper_parameters = initial_hyper_parameters\r\n",
        "\r\n",
        "        # These are the parameters we want to use with population based training\r\n",
        "        self.actor_learning_rate = self.hyper_parameters['actor_learning_rate']\r\n",
        "        self.critic_learning_rate = self.hyper_parameters['critic_learning_rate']\r\n",
        "\r\n",
        "        # We're going to use one network for all of our minions\r\n",
        "        self.actor_network = ActorNetwork(output_dims=output_dims, id=self.id)\r\n",
        "        self.critic_network = CriticNetwork(output_dims=1, id=self.id)\r\n",
        "\r\n",
        "        self.actor_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.actor_learning_rate))\r\n",
        "        self.critic_network.compile(optimizer=tf.keras.optimizers.RMSprop(learning_rate=self.critic_learning_rate))\r\n",
        "\r\n",
        "        # Since Actor-Critic is an on-policy method, we will not use a replay buffer\r\n",
        "        self.states = []\r\n",
        "        self.actions = []\r\n",
        "        self.rewards = []\r\n",
        "        self.episode_rewards = []\r\n",
        "        self.scores = []\r\n",
        "        self.actor_losses = []\r\n",
        "        self.critic_losses = []\r\n",
        "\r\n",
        "    def save_models(self):\r\n",
        "        # print('... saving models ...')\r\n",
        "        self.actor_network.save_weights(self.actor_network.checkpoint_file)\r\n",
        "        self.critic_network.save_weights(self.critic_network.checkpoint_file)\r\n",
        "\r\n",
        "    def load_models(self):\r\n",
        "        # print('... loading models ...')\r\n",
        "        self.actor_network.load_weights(self.actor_network.checkpoint_file)\r\n",
        "        self.critic_network.load_weights(self.critic_network.checkpoint_file)\r\n",
        "\r\n",
        "    def choose_action(self, state):\r\n",
        "        action_logits = self.actor_network(tf.convert_to_tensor([state]))\r\n",
        "        action_probabilities = tf.nn.softmax(action_logits)\r\n",
        "        action_distribution = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\r\n",
        "        action = action_distribution.sample()\r\n",
        "\r\n",
        "        return int(action.numpy()[0])\r\n",
        "\r\n",
        "    def learn(self):\r\n",
        "        discounted_rewards = []\r\n",
        "        sum_reward = 0\r\n",
        "        self.rewards.reverse()\r\n",
        "        for r in self.rewards:\r\n",
        "            sum_reward = r + self.hyper_parameters['discount_factor'] * sum_reward\r\n",
        "            discounted_rewards.append(sum_reward)\r\n",
        "        discounted_rewards.reverse()\r\n",
        "\r\n",
        "        with tf.GradientTape() as tape1, tf.GradientTape() as tape2:\r\n",
        "            # Start calculating the Actor and Critic losses for each minion's experience\r\n",
        "            action_logits = self.actor_network(tf.convert_to_tensor(self.states))\r\n",
        "            state_values = self.critic_network(tf.convert_to_tensor(self.states))\r\n",
        "            action_probabilities = tf.nn.softmax(action_logits)\r\n",
        "            # We'll be using an advantage function\r\n",
        "            action_distributions = tfp.distributions.Categorical(probs=action_probabilities, dtype=tf.float32)\r\n",
        "            log_probs = action_distributions.log_prob(self.actions)\r\n",
        "            advantage = tf.math.subtract(discounted_rewards, state_values)\r\n",
        "            entropy = -1 * tf.math.reduce_sum(action_probabilities * tf.math.log(action_probabilities))\r\n",
        "            actor_loss = tf.math.reduce_mean(-1 * log_probs * advantage) - self.hyper_parameters[\r\n",
        "                'entropy_coefficient'] * entropy\r\n",
        "            critic_loss = tf.math.reduce_mean(advantage ** 2)\r\n",
        "\r\n",
        "            # Optimize master's network with the mean of all the losses\r\n",
        "        actor_grads = tape1.gradient(actor_loss, self.actor_network.trainable_variables)\r\n",
        "        critic_grads = tape2.gradient(critic_loss, self.critic_network.trainable_variables)\r\n",
        "        self.actor_network.optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\r\n",
        "        self.critic_network.optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\r\n",
        "        self.actor_losses.append(actor_loss.numpy())\r\n",
        "        self.critic_losses.append(critic_loss.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewaKLDdzTmAi"
      },
      "source": [
        "# Coordinator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkI73wBFTowU"
      },
      "source": [
        "class Coordinator:\r\n",
        "    def __init__(self, environment_name, initial_hyper_parameters, coordinator_id, log_file_name):\r\n",
        "        self.environment_name = environment_name\r\n",
        "        self.id = coordinator_id\r\n",
        "        self.log_file_name = log_file_name\r\n",
        "        self.environment = simple_adversary_v2.env(N=2, max_cycles=100)\r\n",
        "        self.observation = self.environment.reset()\r\n",
        "        self.number_of_agents = len(self.environment.agents)\r\n",
        "\r\n",
        "        self.hyper_parameters = initial_hyper_parameters\r\n",
        "        self.agents = [Agent(self.environment.action_spaces[i].n, self.hyper_parameters, id=i) for i in\r\n",
        "                       self.environment.action_spaces.keys()]\r\n",
        "\r\n",
        "        self.episode_finished = False\r\n",
        "        self.episode_num = 0\r\n",
        "        self.mean_scores = 0\r\n",
        "        self.episode_number = 0\r\n",
        "        self.episode_rewards = []\r\n",
        "        self.total_reward = 0\r\n",
        "        self.episode_finished = False\r\n",
        "        self.steps = 1\r\n",
        "\r\n",
        "    def play(self, show_env=False):\r\n",
        "        self.episode_finished = False\r\n",
        "        self.environment.reset()\r\n",
        "        while not all(done == True for done in self.environment.dones):\r\n",
        "            i = 0\r\n",
        "            for agent in self.environment.agent_iter():\r\n",
        "\r\n",
        "                agent_obs, agent_reward, agent_done, agent_info = self.environment.last()\r\n",
        "                self.agents[i].rewards.append(agent_reward)\r\n",
        "\r\n",
        "                self.agents[i].states.append(agent_obs)\r\n",
        "                temp_action = self.agents[i].choose_action(agent_obs)\r\n",
        "\r\n",
        "                if agent_done:\r\n",
        "                    self.agents[i].actions.append(0)\r\n",
        "                    self.environment.step(None)\r\n",
        "                else:\r\n",
        "                    self.agents[i].actions.append(temp_action)\r\n",
        "                    self.environment.step(temp_action)\r\n",
        "\r\n",
        "                if show_env:\r\n",
        "                    self.environment.render(mode='human')\r\n",
        "                i += 1\r\n",
        "                if i == self.number_of_agents: i = 0\r\n",
        "\r\n",
        "        self.episode_num += 1\r\n",
        "        total_episode_reward = 0\r\n",
        "        for agent in self.agents[:1]:\r\n",
        "            total_episode_reward += np.sum(agent.rewards)\r\n",
        "        coordinator.episode_rewards.append(total_episode_reward)\r\n",
        "        for agent in self.agents:\r\n",
        "            f = open(f'{self.environment_name}-{self.log_file_name}.csv', 'a')\r\n",
        "            f.write(\r\n",
        "                f'{self.id},{self.episode_num},{total_episode_reward},{agent.id},{np.sum(agent.rewards)},'\r\n",
        "                f'{agent.hyper_parameters[\"actor_learning_rate\"]},{agent.hyper_parameters[\"critic_learning_rate\"]},'\r\n",
        "                f'{agent.hyper_parameters[\"entropy_coefficient\"]}\\n')\r\n",
        "            f.close()\r\n",
        "\r\n",
        "        for agent in self.agents:\r\n",
        "            agent.learn()\r\n",
        "            agent.states.clear()\r\n",
        "            agent.rewards.clear()\r\n",
        "            agent.actions.clear()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpFOf37kT0cg"
      },
      "source": [
        "# PBT Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpUSV6v5T3Qt"
      },
      "source": [
        "def exploit(population):\r\n",
        "    sorted_population = sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)\r\n",
        "    best_coordinators = sorted_population[:3]\r\n",
        "    worst_coordinators = sorted_population[-3:]\r\n",
        "\r\n",
        "    # for each other agent, load their models here\r\n",
        "    for coordinator in worst_coordinators:\r\n",
        "        worst_coordinator_id = coordinator.id\r\n",
        "        worst_coordinator_episode = coordinator.episode_num\r\n",
        "        new_coordinator = copy.deepcopy(random.choice(best_coordinators))\r\n",
        "        print(f'Agent -> {new_coordinator.id} will replace {worst_coordinator_id}')\r\n",
        "        new_coordinator.id = worst_coordinator_id\r\n",
        "        new_coordinator.episode_num = worst_coordinator_episode\r\n",
        "        population.remove(coordinator)\r\n",
        "        population.append(new_coordinator)\r\n",
        "        explore(new_coordinator)\r\n",
        "\r\n",
        "    for coordinator in population:\r\n",
        "        coordinator.episode_rewards.clear()\r\n",
        "\r\n",
        "\r\n",
        "def explore(coordinator):\r\n",
        "    for agent in coordinator.agents:\r\n",
        "        new_actor_learning_rate = round(agent.hyper_parameters['actor_learning_rate'] * random.uniform(0.8, 1.2), 6)\r\n",
        "        new_critic_learning_rate = round(agent.hyper_parameters['critic_learning_rate'] * random.uniform(0.8, 1.2), 6)\r\n",
        "\r\n",
        "        agent.actor_network.optimizer.learning_rate.assign(new_actor_learning_rate)\r\n",
        "        agent.critic_network.optimizer.learning_rate.assign(new_critic_learning_rate)\r\n",
        "        agent.hyper_parameters['actor_learning_rate'] = new_actor_learning_rate\r\n",
        "        agent.hyper_parameters['critic_learning_rate'] = new_critic_learning_rate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KebZ1Qr4T6Tw"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4DRcPJLT8Vv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b687ec7c-a0d8-4c51-e9cc-c88abc7a5126"
      },
      "source": [
        "environment_name = 'Simple Adevrsary'\r\n",
        "log_file_name = 'PBT-5'\r\n",
        "population = []\r\n",
        "for i in range(16):\r\n",
        "    population.append(Coordinator(environment_name,\r\n",
        "                                  {'actor_learning_rate': round(random.uniform(0.00001, 0.001), 4),\r\n",
        "                                   'critic_learning_rate': round(random.uniform(0.00001, 0.001), 4),\r\n",
        "                                   'entropy_coefficient': 0.0001,\r\n",
        "                                   'critic_coefficient': 0.3,\r\n",
        "                                   'discount_factor': 0.95,\r\n",
        "                                   'unroll_length': 5,\r\n",
        "                                   'minions_num': 5},\r\n",
        "                                  coordinator_id=i, log_file_name=log_file_name))\r\n",
        "f = open(f'{environment_name}-{log_file_name}.csv', 'a')\r\n",
        "f.write(\r\n",
        "    f'Coordinator ID,Episode Number,Episode Reward,Agent ID,Agent Reward, Actor Learning Rate, Critic Learning Rate, '\r\n",
        "    f'Entropy\\n')\r\n",
        "f.close()\r\n",
        "j = 0\r\n",
        "\r\n",
        "for j in range(1, 1501):\r\n",
        "    for coordinator in population:\r\n",
        "        try:\r\n",
        "          coordinator.play(show_env=False)\r\n",
        "        except Exception:\r\n",
        "            new_coordinator_id = coordinator.id\r\n",
        "            new_coordinator_episode = coordinator.episode_num\r\n",
        "            population.remove(coordinator)\r\n",
        "            new_coordinator = copy.deepcopy(\r\n",
        "                random.choice(sorted(population, key=lambda i: np.mean(i.episode_rewards), reverse=True)[:3]))\r\n",
        "            new_coordinator.id = new_coordinator_id\r\n",
        "            new_coordinator.episode_num = new_coordinator_episode\r\n",
        "            population.append(new_coordinator)\r\n",
        "\r\n",
        "    if j % 100 == 0:\r\n",
        "        for coordinator in population:\r\n",
        "            print(f'{coordinator.id} --> {coordinator.episode_num} --> {np.mean(coordinator.episode_rewards)}')\r\n",
        "        exploit(population)\r\n",
        "        # files.download(f'{environment_name}-{log_file_name}.csv')\r\n",
        "\r\n",
        "files.download(f'{environment_name}-{log_file_name}.csv')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 --> 98 --> -182.92588807425722\n",
            "4 --> 99 --> -141.74025349893844\n",
            "5 --> 100 --> -121.07847523389326\n",
            "8 --> 98 --> -125.64035183082399\n",
            "14 --> 97 --> -126.34846558292618\n",
            "15 --> 100 --> -246.05297029953147\n",
            "7 --> 100 --> -132.98553960969187\n",
            "12 --> 100 --> -128.4342438942282\n",
            "1 --> 100 --> -177.99126402328747\n",
            "13 --> 99 --> -111.69903586673288\n",
            "11 --> 97 --> -114.88035969430422\n",
            "10 --> 99 --> -111.30940036858692\n",
            "6 --> 99 --> -113.57753837084731\n",
            "3 --> 100 --> -119.10459578604016\n",
            "9 --> 100 --> -113.47350985590235\n",
            "0 --> 100 --> -111.30940036858692\n",
            "Agent -> 10 will replace 1\n",
            "Agent -> 10 will replace 2\n",
            "Agent -> 10 will replace 15\n",
            "14 --> 196 --> -145.42891437622495\n",
            "7 --> 200 --> -76.15401225376102\n",
            "10 --> 197 --> -100.80445619195041\n",
            "6 --> 199 --> -99.69169473720277\n",
            "15 --> 195 --> -87.09201408161682\n",
            "3 --> 199 --> -140.8816089336075\n",
            "2 --> 197 --> -99.82345809960222\n",
            "4 --> 198 --> -93.65217741905327\n",
            "11 --> 195 --> -84.97706990667635\n",
            "13 --> 199 --> -79.09314642830387\n",
            "9 --> 197 --> -77.52250222001575\n",
            "1 --> 199 --> -78.01316466882312\n",
            "8 --> 196 --> -79.63216212075402\n",
            "12 --> 200 --> -76.8742930123699\n",
            "5 --> 198 --> -76.12684174438945\n",
            "0 --> 199 --> -79.74000618007145\n",
            "Agent -> 5 will replace 10\n",
            "Agent -> 5 will replace 3\n",
            "Agent -> 7 will replace 14\n",
            "7 --> 300 --> -103.84092856614872\n",
            "15 --> 294 --> -100.3535692286717\n",
            "2 --> 297 --> -108.96562538989929\n",
            "11 --> 294 --> -90.86329161063486\n",
            "13 --> 299 --> -99.53215388558506\n",
            "9 --> 297 --> -106.70852976249267\n",
            "1 --> 299 --> -90.7720553888795\n",
            "8 --> 296 --> -97.80032238499781\n",
            "12 --> 300 --> -111.85699003364958\n",
            "3 --> 297 --> -105.20769458550937\n",
            "0 --> 298 --> -72.34250924417798\n",
            "6 --> 298 --> -74.42762062341784\n",
            "14 --> 295 --> -72.83845931992364\n",
            "10 --> 296 --> -75.24599946613154\n",
            "4 --> 298 --> -72.41927955214783\n",
            "5 --> 298 --> -72.51053259760002\n",
            "Agent -> 0 will replace 9\n",
            "Agent -> 5 will replace 2\n",
            "Agent -> 0 will replace 12\n",
            "13 --> 397 --> -83.16864910943264\n",
            "1 --> 399 --> -99.00852639919684\n",
            "8 --> 396 --> -98.06845210683336\n",
            "6 --> 395 --> -76.8919253818071\n",
            "4 --> 397 --> -65.27537554115636\n",
            "12 --> 394 --> -67.11814838339505\n",
            "11 --> 392 --> -76.27039099477581\n",
            "2 --> 395 --> -72.3369155743423\n",
            "9 --> 393 --> -67.19488960206918\n",
            "10 --> 392 --> -60.33180348921716\n",
            "3 --> 395 --> -60.288394139151784\n",
            "15 --> 389 --> -64.78414944456749\n",
            "0 --> 395 --> -71.62046453778034\n",
            "7 --> 398 --> -61.03032880240366\n",
            "5 --> 395 --> -61.28939925921252\n",
            "14 --> 391 --> -59.98242236448303\n",
            "Agent -> 3 will replace 13\n",
            "Agent -> 3 will replace 8\n",
            "Agent -> 10 will replace 1\n",
            "6 --> 495 --> -93.15700324493473\n",
            "9 --> 489 --> -85.44352627885074\n",
            "1 --> 495 --> -67.4332161209246\n",
            "11 --> 490 --> -77.41577438588754\n",
            "8 --> 494 --> -74.57037210246237\n",
            "4 --> 496 --> -82.02803922020716\n",
            "7 --> 497 --> -67.06001775704972\n",
            "12 --> 491 --> -68.01013408545825\n",
            "0 --> 491 --> -67.95792935495673\n",
            "3 --> 491 --> -67.09423386986717\n",
            "10 --> 489 --> -71.40697890720111\n",
            "14 --> 490 --> -72.60646383035723\n",
            "13 --> 491 --> -68.24292994552606\n",
            "2 --> 491 --> -69.06750353584019\n",
            "5 --> 493 --> -68.48806584919677\n",
            "15 --> 483 --> -67.04189509093099\n",
            "Agent -> 15 will replace 4\n",
            "Agent -> 3 will replace 9\n",
            "Agent -> 7 will replace 6\n",
            "2 --> 588 --> -72.5967193252457\n",
            "9 --> 586 --> -66.10319265571357\n",
            "6 --> 591 --> -73.73619846866518\n",
            "7 --> 597 --> -65.18011154803912\n",
            "5 --> 591 --> -70.30200825589674\n",
            "12 --> 589 --> -64.52598305073909\n",
            "15 --> 580 --> -64.85989500380724\n",
            "0 --> 589 --> -67.09252785924319\n",
            "3 --> 589 --> -66.3067071365604\n",
            "13 --> 590 --> -65.95782674792346\n",
            "4 --> 593 --> -66.2031571120832\n",
            "10 --> 587 --> -66.86746595325616\n",
            "1 --> 595 --> -64.6581903143634\n",
            "11 --> 589 --> -65.05903604423712\n",
            "14 --> 588 --> -65.02045495779471\n",
            "8 --> 592 --> -65.05552738123647\n",
            "Agent -> 1 will replace 5\n",
            "Agent -> 12 will replace 2\n",
            "Agent -> 15 will replace 6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py:3373: RuntimeWarning: Mean of empty slice.\n",
            "  out=out, **kwargs)\n",
            "/usr/local/lib/python3.6/dist-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
            "  ret = ret.dtype.type(ret / rcount)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14 --> 685 --> -64.0121013388919\n",
            "5 --> 690 --> -70.21724541868552\n",
            "8 --> 691 --> -59.19948180167986\n",
            "9 --> 685 --> -61.091401292854805\n",
            "7 --> 696 --> -59.04332508844286\n",
            "0 --> 689 --> -71.61506819603667\n",
            "10 --> 685 --> -61.66937221023762\n",
            "13 --> 690 --> -58.12622760272357\n",
            "2 --> 687 --> -63.99825025065179\n",
            "11 --> 687 --> -59.371066833868234\n",
            "15 --> 680 --> -60.46968906735388\n",
            "4 --> 692 --> -57.772226937033764\n",
            "3 --> 687 --> -59.25835808702907\n",
            "12 --> 688 --> -58.550021326884654\n",
            "1 --> 693 --> -58.47620738358729\n",
            "6 --> 690 --> -58.35727226713245\n",
            "Agent -> 6 will replace 14\n",
            "Agent -> 13 will replace 5\n",
            "Agent -> 13 will replace 0\n",
            "8 --> 791 --> -70.73529348172531\n",
            "9 --> 785 --> -69.65668477566642\n",
            "7 --> 796 --> -67.45903244897605\n",
            "4 --> 790 --> -63.736323015069814\n",
            "12 --> 787 --> -77.62547384410715\n",
            "5 --> 716 --> -56.37647253878333\n",
            "0 --> 744 --> -56.37647253878333\n",
            "3 --> 734 --> -56.37647253878333\n",
            "6 --> 715 --> -56.37647253878333\n",
            "15 --> 757 --> -56.37647253878333\n",
            "10 --> 711 --> -56.37647253878333\n",
            "2 --> 744 --> -56.37647253878333\n",
            "13 --> 737 --> -56.37647253878333\n",
            "14 --> 742 --> -56.37647253878333\n",
            "11 --> 772 --> -56.37647253878333\n",
            "1 --> 719 --> -56.37647253878333\n",
            "Agent -> 5 will replace 9\n",
            "Agent -> 5 will replace 8\n",
            "Agent -> 0 will replace 12\n",
            "4 --> 889 --> -64.47888976053093\n",
            "1 --> 818 --> -80.79135195595636\n",
            "8 --> 890 --> -64.99515547536154\n",
            "10 --> 806 --> -76.23457014758826\n",
            "0 --> 842 --> -67.17027541831827\n",
            "9 --> 883 --> -72.26165238833644\n",
            "12 --> 883 --> -71.36173821437626\n",
            "13 --> 834 --> -86.33057679639685\n",
            "15 --> 856 --> -74.02269771933418\n",
            "6 --> 811 --> -80.16495469716115\n",
            "14 --> 838 --> -69.1845292395215\n",
            "11 --> 861 --> -60.96238798570297\n",
            "3 --> 814 --> -60.96238798570297\n",
            "7 --> 880 --> -60.96238798570297\n",
            "5 --> 797 --> -60.96238798570297\n",
            "2 --> 827 --> -60.96238798570297\n",
            "Agent -> 3 will replace 6\n",
            "Agent -> 7 will replace 1\n",
            "Agent -> 11 will replace 13\n",
            "15 --> 954 --> -69.52722943109768\n",
            "11 --> 959 --> -72.7173085512653\n",
            "2 --> 926 --> -73.02141276381305\n",
            "5 --> 892 --> -87.2236916811149\n",
            "6 --> 909 --> -76.7222239851998\n",
            "13 --> 931 --> -79.87143563955533\n",
            "12 --> 981 --> -78.33611092628378\n",
            "3 --> 912 --> -68.2223150111732\n",
            "0 --> 941 --> -82.5463352616273\n",
            "9 --> 979 --> -76.8381126083555\n",
            "7 --> 979 --> -66.2781917671099\n",
            "10 --> 905 --> -76.58252854759697\n",
            "4 --> 989 --> -78.98494966403427\n",
            "8 --> 987 --> -80.32079934819379\n",
            "14 --> 938 --> -78.30095185826838\n",
            "1 --> 918 --> -76.03655607488385\n",
            "Agent -> 7 will replace 8\n",
            "Agent -> 7 will replace 0\n",
            "Agent -> 3 will replace 5\n",
            "15 --> 1054 --> -65.57319909111658\n",
            "11 --> 1059 --> -73.08950395333397\n",
            "2 --> 1026 --> -95.84661148675055\n",
            "6 --> 1009 --> -84.70985513770522\n",
            "13 --> 1031 --> -83.84311322593256\n",
            "12 --> 1081 --> -77.7152610821608\n",
            "3 --> 1012 --> -71.27672723571962\n",
            "7 --> 1078 --> -86.75927684260594\n",
            "10 --> 1005 --> -81.2949322117047\n",
            "4 --> 1089 --> -67.09842072923472\n",
            "14 --> 1038 --> -93.9678408729639\n",
            "8 --> 1086 --> -71.24909484531086\n",
            "0 --> 1041 --> -58.62907737476112\n",
            "5 --> 992 --> -89.36443648551395\n",
            "9 --> 1079 --> -73.1954153293488\n",
            "1 --> 1018 --> -58.32460653865513\n",
            "Agent -> 1 will replace 5\n",
            "Agent -> 1 will replace 14\n",
            "Agent -> 0 will replace 2\n",
            "15 --> 1154 --> -60.48101005544396\n",
            "11 --> 1159 --> -65.70267286113113\n",
            "13 --> 1130 --> -66.46396394148974\n",
            "12 --> 1181 --> -63.61791048789948\n",
            "3 --> 1112 --> -54.14829082212416\n",
            "4 --> 1187 --> -55.776421658771454\n",
            "8 --> 1186 --> -67.84605635648899\n",
            "0 --> 1141 --> -63.43363378263891\n",
            "1 --> 1117 --> -60.92847040979908\n",
            "5 --> 1092 --> -57.19639500787901\n",
            "14 --> 1138 --> -63.112754209981304\n",
            "2 --> 1126 --> -56.19430637555157\n",
            "9 --> 1179 --> -62.57314184078237\n",
            "6 --> 1109 --> -62.448188791763585\n",
            "10 --> 1105 --> -54.341156968610036\n",
            "7 --> 1178 --> -56.67671533259231\n",
            "Agent -> 10 will replace 11\n",
            "Agent -> 10 will replace 13\n",
            "Agent -> 3 will replace 8\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}